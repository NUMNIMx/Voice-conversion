import streamlit as st
import soundfile as sf
from pydub import AudioSegment
import os
import torch
import torchaudio
import torch.nn as nn
import zipfile
import shutil

# ‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤ CSS ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏ß‡∏¢‡∏á‡∏≤‡∏°
st.markdown("""
    <style>
    .main {
        background-color: #f0f2f6;
        padding: 20px;
    }
    .stButton>button {
        background-color: #4CAF50;
        color: white;
        border-radius: 5px;
    }
    .stSuccess {
        background-color: #dff0d8;
        color: #3c763d;
    }
    .stWarning {
        background-color: #fcf8e3;
        color: #8a6d3b;
    }
    .stError {
        background-color: #f2dede;
        color: #a94442;
    }
    </style>
""", unsafe_allow_html=True)

# ‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡πÅ‡∏õ‡∏•‡∏á MP3 ‡πÄ‡∏õ‡πá‡∏ô WAV
def convert_mp3_to_wav(mp3_file, wav_path):
    audio = AudioSegment.from_mp3(mp3_file)
    audio.export(wav_path, format="wav")
    return wav_path

# ‡πÇ‡∏´‡∏•‡∏î Hubert model
hubert_model = torchaudio.models.hubert_base()
hubert_model.eval()

# ‡∏ô‡∏¥‡∏¢‡∏≤‡∏° RVCModel
class RVCModel(nn.Module):
    def __init__(self):
        super(RVCModel, self).__init__()
        self.linear = nn.Linear(768, 80)  # Hubert feature (768) -> mel bins (80)

    def forward(self, x):
        return self.linear(x)  # [seq_len, 768] -> [seq_len, 80]

# ‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡πÄ‡∏ó‡∏£‡∏ô‡πÇ‡∏°‡πÄ‡∏î‡∏•
def train_model(dataset_zip_path, epochs=1):
    temp_dir = "temp_dataset"
    if os.path.exists(temp_dir):
        shutil.rmtree(temp_dir)
    os.makedirs(temp_dir)
    with zipfile.ZipFile(dataset_zip_path, 'r') as zip_ref:
        zip_ref.extractall(temp_dir)

    source_dir = os.path.join(temp_dir, 'source')
    target_dir = os.path.join(temp_dir, 'target')
    source_files = sorted([f for f in os.listdir(source_dir) if f.endswith('.wav')])
    target_files = sorted([f for f in os.listdir(target_dir) if f.endswith('.wav')])

    if not source_files or not target_files or len(source_files) != len(target_files):
        raise ValueError("‡πÑ‡∏ü‡∏•‡πå‡πÉ‡∏ô source ‡πÅ‡∏•‡∏∞ target ‡πÑ‡∏°‡πà‡∏ï‡∏£‡∏á‡∏Å‡∏±‡∏ô‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà‡∏°‡∏µ‡πÑ‡∏ü‡∏•‡πå .wav")

    model = RVCModel()
    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)
    criterion = nn.MSELoss()

    progress_bar = st.progress(0)
    for epoch in range(epochs):
        for i, (src_file, tgt_file) in enumerate(zip(source_files, target_files)):
            src_path = os.path.join(source_dir, src_file)
            tgt_path = os.path.join(target_dir, tgt_file)

            src_waveform, src_sr = torchaudio.load(src_path)
            tgt_waveform, tgt_sr = torchaudio.load(tgt_path)

            if src_sr != 16000:
                src_waveform = torchaudio.functional.resample(src_waveform, src_sr, 16000)
            if tgt_sr != 16000:
                tgt_waveform = torchaudio.functional.resample(tgt_waveform, tgt_sr, 16000)

            with torch.no_grad():
                src_features = hubert_model(src_waveform).last_hidden_state[0]  # [seq_len, 768]

            mel_transform = torchaudio.transforms.MelSpectrogram(
                sample_rate=16000, n_fft=1024, hop_length=320, n_mels=80
            )
            tgt_mel = mel_transform(tgt_waveform).squeeze(0).T  # [num_frames, 80]

            min_len = min(src_features.shape[0], tgt_mel.shape[0])
            src_features = src_features[:min_len]
            tgt_mel = tgt_mel[:min_len]

            optimizer.zero_grad()
            predicted_mel = model(src_features)
            loss = criterion(predicted_mel, tgt_mel)
            loss.backward()
            optimizer.step()

            progress_bar.progress((i + 1) / len(source_files))

        st.write(f"Epoch {epoch+1}/{epochs} ‡πÄ‡∏™‡∏£‡πá‡∏à‡∏™‡∏¥‡πâ‡∏ô")

    model_path = "trained_model.pth"
    torch.save(model.state_dict(), model_path)
    shutil.rmtree(temp_dir)  # ‡∏•‡πâ‡∏≤‡∏á temporary files
    return model_path

# ‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô‡πÇ‡∏°‡πÄ‡∏î‡∏•
def use_model(model, source_path):
    source_waveform, sr = torchaudio.load(source_path)
    if sr != 16000:
        source_waveform = torchaudio.functional.resample(source_waveform, sr, 16000)
        sr = 16000

    with torch.no_grad():
        source_features = hubert_model(source_waveform).last_hidden_state[0]  # [seq_len, 768]
        predicted_mel = model(source_features)  # [seq_len, 80]

    griffin_lim = torchaudio.transforms.GriffinLim(n_fft=1024, hop_length=320, n_iter=30)
    generated_waveform = griffin_lim(predicted_mel.T)  # [time]
    return generated_waveform.numpy(), sr

# Streamlit GUI
st.title("üéôÔ∏è Voice Cloning with RVC v2")
st.markdown("‡πÇ‡∏õ‡∏£‡πÅ‡∏Å‡∏£‡∏°‡∏ô‡∏µ‡πâ‡∏ä‡πà‡∏ß‡∏¢‡∏Ñ‡∏∏‡∏ì‡πÄ‡∏ó‡∏£‡∏ô‡πÅ‡∏•‡∏∞‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏Å‡∏≤‡∏£‡πÅ‡∏õ‡∏•‡∏á‡πÄ‡∏™‡∏µ‡∏¢‡∏á‡πÑ‡∏î‡πâ‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏á‡πà‡∏≤‡∏¢‡∏î‡∏≤‡∏¢!")

# ‡∏ï‡∏±‡∏ß‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡πÇ‡∏´‡∏°‡∏î
mode = st.radio("‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡πÇ‡∏´‡∏°‡∏î‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏á‡∏≤‡∏ô:", ("Train Model", "Use Model"), horizontal=True)

if mode == "Train Model":
    st.subheader("üõ†Ô∏è ‡πÇ‡∏´‡∏°‡∏î‡πÄ‡∏ó‡∏£‡∏ô‡πÇ‡∏°‡πÄ‡∏î‡∏•")
    with st.expander("‡∏Ñ‡∏≥‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥"):
        st.write("""
            - ‡∏≠‡∏±‡∏õ‡πÇ‡∏´‡∏•‡∏î‡πÑ‡∏ü‡∏•‡πå `.zip` ‡∏ó‡∏µ‡πà‡∏°‡∏µ‡πÇ‡∏ü‡∏•‡πÄ‡∏î‡∏≠‡∏£‡πå `source` ‡πÅ‡∏•‡∏∞ `target`
            - ‡πÑ‡∏ü‡∏•‡πå‡πÉ‡∏ô `source` ‡πÅ‡∏•‡∏∞ `target` ‡∏ï‡πâ‡∏≠‡∏á‡πÄ‡∏õ‡πá‡∏ô `.wav` ‡πÅ‡∏•‡∏∞‡∏°‡∏µ‡∏ä‡∏∑‡πà‡∏≠‡πÄ‡∏´‡∏°‡∏∑‡∏≠‡∏ô‡∏Å‡∏±‡∏ô
            - ‡∏Å‡∏≤‡∏£‡πÄ‡∏ó‡∏£‡∏ô‡∏≠‡∏≤‡∏à‡πÉ‡∏ä‡πâ‡πÄ‡∏ß‡∏•‡∏≤‡∏ô‡∏≤‡∏ô ‡∏Ç‡∏∂‡πâ‡∏ô‡∏≠‡∏¢‡∏π‡πà‡∏Å‡∏±‡∏ö‡∏Ç‡∏ô‡∏≤‡∏î dataset ‡πÅ‡∏•‡∏∞‡∏à‡∏≥‡∏ô‡∏ß‡∏ô epoch
        """)

    col1, col2 = st.columns([2, 1])
    with col1:
        dataset_zip = st.file_uploader("‡∏≠‡∏±‡∏õ‡πÇ‡∏´‡∏•‡∏î dataset (.zip)", type=["zip"])
    with col2:
        epochs = st.number_input("‡∏à‡∏≥‡∏ô‡∏ß‡∏ô epochs", min_value=1, value=1, step=1)

    if st.button("‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏Å‡∏≤‡∏£‡πÄ‡∏ó‡∏£‡∏ô"):
        if dataset_zip:
            with open("dataset.zip", "wb") as f:
                f.write(dataset_zip.read())
            try:
                model_path = train_model("dataset.zip", epochs=epochs)
                st.success("‡πÄ‡∏ó‡∏£‡∏ô‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à!")
                with open(model_path, "rb") as f:
                    st.download_button("‡∏î‡∏≤‡∏ß‡∏ô‡πå‡πÇ‡∏´‡∏•‡∏î‡πÇ‡∏°‡πÄ‡∏î‡∏• (.pth)", f, file_name="trained_model.pth")
            except Exception as e:
                st.error(f"‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î: {str(e)}")
        else:
            st.error("‡∏Å‡∏£‡∏∏‡∏ì‡∏≤‡∏≠‡∏±‡∏õ‡πÇ‡∏´‡∏•‡∏î dataset")

elif mode == "Use Model":
    st.subheader("üéµ ‡πÇ‡∏´‡∏°‡∏î‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô‡πÇ‡∏°‡πÄ‡∏î‡∏•")
    with st.expander("‡∏Ñ‡∏≥‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥"):
        st.write("""
            - ‡∏≠‡∏±‡∏õ‡πÇ‡∏´‡∏•‡∏î‡πÑ‡∏ü‡∏•‡πå‡πÇ‡∏°‡πÄ‡∏î‡∏• `.pth` ‡∏ó‡∏µ‡πà‡πÄ‡∏ó‡∏£‡∏ô‡πÅ‡∏•‡πâ‡∏ß
            - ‡∏≠‡∏±‡∏õ‡πÇ‡∏´‡∏•‡∏î‡πÑ‡∏ü‡∏•‡πå `source audio` (`.mp3` ‡∏´‡∏£‡∏∑‡∏≠ `.wav`)
            - ‡∏Å‡∏î‡∏õ‡∏∏‡πà‡∏°‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÄ‡∏™‡∏µ‡∏¢‡∏á‡∏ó‡∏µ‡πà‡πÅ‡∏õ‡∏•‡∏á‡πÅ‡∏•‡πâ‡∏ß
        """)

    col1, col2 = st.columns(2)
    with col1:
        model_file = st.file_uploader("‡∏≠‡∏±‡∏õ‡πÇ‡∏´‡∏•‡∏î‡πÇ‡∏°‡πÄ‡∏î‡∏• (.pth)", type=["pth"])
    with col2:
        source_audio = st.file_uploader("‡∏≠‡∏±‡∏õ‡πÇ‡∏´‡∏•‡∏î source audio", type=["mp3", "wav"])

    if model_file and source_audio:
        with open("model.pth", "wb") as f:
            f.write(model_file.read())
        model = RVCModel()
        model.load_state_dict(torch.load("model.pth", map_location=torch.device('cpu')))
        model.eval()

        if source_audio.name.endswith('.mp3'):
            source_path = convert_mp3_to_wav(source_audio, "source.wav")
        else:
            with open("source.wav", "wb") as f:
                f.write(source_audio.read())
            source_path = "source.wav"

        if st.button("‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÄ‡∏™‡∏µ‡∏¢‡∏á‡∏ó‡∏µ‡πà‡πÅ‡∏õ‡∏•‡∏á‡πÅ‡∏•‡πâ‡∏ß"):
            try:
                generated_audio, sr = use_model(model, source_path)
                sf.write("generated_audio.wav", generated_audio, sr)
                st.audio("generated_audio.wav")
                st.success("‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÑ‡∏ü‡∏•‡πå‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à!")
            except Exception as e:
                st.error(f"‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î: {str(e)}")
    else:
        st.warning("‡∏Å‡∏£‡∏∏‡∏ì‡∏≤‡∏≠‡∏±‡∏õ‡πÇ‡∏´‡∏•‡∏î‡πÇ‡∏°‡πÄ‡∏î‡∏•‡πÅ‡∏•‡∏∞ source audio")